---
title: "Social_Networks_Second_Assignment"
format: pdf
date: "2025-05-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Requierements

You will have to present a report: • Include a section in which you address each of the steps in point 3. and answer the questions. • You have to show the code used to solve the question and the answer. • Hand out the pdf or html version of the notebook

# Introduction

escorts — Brazilian prostitution network (2010)

Description

:   A bipartite network of escort and individuals who buy sex from them in Brazil, extracted from a Brazilian online community for such ratings. Edges represent a purchase of sexual intercourse with an escort, and the edge weight gives the buyer's rating of the escort: -1 (bad), 0 (neutral), or +1 (good). Other edge attributes are available.

<https://networks.skewed.de/net/escorts>

L. Rocha, F. Liljeros, & P. Holme. "Simulated epidemics in an empirical spatiotemporal network of 50,185 sexual contacts." PLoS Comput Biol, 7(3). (2011)., <https://doi.org/10.1371/journal.pcbi.1001109> \[[\@sci-hub](https://sci-hub.st/10.1371/journal.pcbi.1001109)\]

## Libraries

```{r}
library(readr)              # read csv 
library(igraph)             # create graph
```

## Network

We load the edges and nodes and create the graph from the data.

```{r loading network}
edges <- read_csv("network_escorts/edges.csv")
head(edges, 4)

nodes <- read_csv("network_escorts/nodes.csv")
head(nodes, 4)

# Buiding graph from data
g <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)
```

# Task: Link Prediction Project

## 1. Delete a fraction of real edges in the network and create a table of those links deleted (positive class) and of links non-present (negative class)

Since we have 16730 nodes we delete $n= ?$ edges from $g$ to obtain $g'$, as (TODO justification for number of nodes deleted).

```{r Gprime}
set.seed(7921)
nlinks <- 5000
indexes_deleted <- sample(1:ecount(g), nlinks) # deletes randomly 5000 links 
# Gprime = this is the network we "observe" = simulates the real-world scenario where some links are missing
Gprime <- delete_edges(g, indexes_deleted)
```

We store the deleted edges in the table `true_edges,` which will be our positive class for our link prediction model, i.e. we will try to predict these links later.

```{r true_edges}
# Build true edges dataframe
true_edges <- data.frame(get.edges(g, indexes_deleted))
```

In `false_edges` we are going to generate non-existent links in the network as our negative class, i.e. node pairs that are not connected in our observed graph (Gprime). We do this because in link prediction we are trying to classify whether a link should exist or not, and to do this we need both positive examples (real edges we deleted) and negative examples (edges that do not exist).

Professors explanation in the notebook:

> To see how well our heuristics perform, we need to compare how well they predict the delete edges with their prediction of non-existent links (our negative class). There are $|V|(|V|-1)/2 -|E|$ ($\gg 1$) non-existent links. From those ones we select only those that happen between highly connected nodes:

//(Technical explanation of code below:)

First, we define well-connected nodes, so that we only sample node-pairs from nodes with a degree greater than 10. We do this because nodes with fewer connections are less likely to form new edges, which would make link prediction more difficult.

We then define an empty dataframe `false_edges` which we will fill up with node-pairs that are not connected.

We sample a number of false edges to collect. In this case, we are sampling 5000 node pairs from our list of well connected nodes. We then check if the two samples nodes are connected in our original graph $g$, and if that is not the case, we add them to `false_edges`. Hence, the condition for a node-pair to be added is that there is no edge between them in the original graph, as we want to represent only missing edges (previously non-existent).

In order to do this 5000 times, we run the previous commands in a `while` loop, in order to ensure that we not only try 5000 times, but that we actually keep looking until we get exactly 5000 false edges in the dataframe.

While this is less efficient that having a `for` loop running 5000 times, we actually ensure that we are not missing any node pairings for all the times that the nodes were connected in the original graph and hence were not integrated into the dataframe.

```{r false_edges}
set.seed(123)

# define well-connected nodes (degree > 10)
most_connected <- which(degree(g)>10)

# empty dataframe to store false edges
false_edges <- data.frame(X1 = integer(), X2 = integer())

# Number of false edges to collect
nlinks <- 5000

# Keep looping until we collect 5000 non-existent edges
while (nrow(false_edges) < nlinks) {
  
  # sampling of two nodes from well_connected list 
  node_pairs <- sample(most_connected, 2)
  node1 <- node_pairs[1]
  node2 <- node_pairs[2]
  
  # Check if they are NOT connected in the original graph
  if (!are_adjacent(g, node1, node2)) {
    
    # If not connected, add them as a "non-existent" link
    false_edges <- rbind(false_edges, data.frame(X1 = node1, X2 = node2))
    
  }
}

```

## 2. Generate a number of proximity/similarty metrics heuristics for each link in the positive and negative class

To compute heuristic scores for similarity/proximity in our positive nad negative class' datasets we firstly bind together the dataframes and add a `obs` variable as a class label to distinguish the real (`obs` = 1) and non-existent (`obs` = 0) links.

```{r total_edges}
## Assign class to true and false edges
true_edges <- data.frame(true_edges, obs=1)
false_edges <- data.frame(false_edges, obs=0)

## Bind them together
total_edges <- rbind(true_edges,false_edges)
colnames(total_edges) <- c("id1","id2","obs")
```

We can perform a sanity check to make sure it worked properly:

```{r}
table(total_edges$obs)
```

As each class has 5000 links, we can safely move on to pre-computing the node neighbourhoods. We do this because our similarity heuristics are going to need this information, hence, if we were to calculate this for every edge in the loop it would be incredibly slow. We optimize our efficiency by having the neighbourhoods already ready to go.

With order=1 of neighborhood, we compute the neighbours of the nodes up to the first order, so direct neighbours (one step away!). We do this for both nodes of the edges.

```{r neighbourhoods}
## Tip: Pre-compute node neighborhoods (which are lists)
n1 <- neighborhood(Gprime,order=1,nodes=total_edges$id1)
n2 <- neighborhood(Gprime,order=1,nodes=total_edges$id2)
```

We are now finally ready to calculate the heuristics for each link. The heuristics are necessary as link prediction deals with estimating the likelihood of a link forming between two nodes, thus we use heuristics to capture different aspects of node similarity or proximity. We can use:

> (TODO: better explanations? not longer but better, these are from chat)
>
> -   Jaccard Similarity: measures the overlap between neighbours.
> -   Adamic-Adar Index: weighs common neighbours inversely by their degree.
> -   Preferential Attachment: assumes that highly connected nodes are more likely to form new links.
>
> These heuristics are popular because they are computationally efficient and capture different structural properties.

Firstly, we initialize three new columns in `total_edges` to store the three heuristic values we will calculate. Then, we use a `for` loop to compute the heuristics for each node-pair in our dataframe. We use the following formulas:

| Heuristic | Formula |
|---------------------------------|--------------------------------------|
| Jaccard Similarity | $$\frac{|A \cap B|}{|A \cup B|}$$ |
| Adamic-Adar Index | $$
\sum_{z \in N(u) \cap N(v)} \frac{1}{\log(\deg(z))}
$$ |
| Preferential Attachment | $$\deg(u) \times \deg(v)$$ |

```{r heuristics}
## Compute heuristics and assign them to total_edges

## Initialize columns in total_edges
total_edges$sim_jacc <- 0
total_edges$sim_aa <- 0
total_edges$sim_pref <- 0

## Compute heuristics for each node pair
for (i in 1:nrow(total_edges)){
  
  # node pair
  node1 <- total_edges$id1[i]
  node2 <- total_edges$id2[i]
  
  # node pair i's neighbors 
  neigh1 <- n1[[i]]
  neigh2 <- n2[[i]]
  
  ## Jaccard similarity = number of common neihbors over the length of common neighbors 
  total_edges$sim_jacc[i] <- length(intersect(neigh1, neigh2)) / length(union(neigh1, neigh2))
  
  ## Adamic-Adar similarity = the sum of 1 over the logarithm of the common neighbors 
  aa_score <- 0
  common_neighbors <- intersect(neigh1, neigh2)
  for (node in common_neighbors){
    degrees_common_neighbors <- degree(Gprime, v = node)
    if (degrees_common_neighbors > 1) {
      aa_score <- aa_score + 1 / log(degrees_common_neighbors)
    }
  }
  total_edges$sim_aa[i] <- aa_score
  
  ## Preferential attachment = simple degree product 
  # degree(g,x)*degree(g,y)
  total_edges$sim_pref[i] <- degree(Gprime, total_edges$id1[i]) * degree(Gprime, total_edges$id2[i])
  
}
```

We can check that the heuristic calculation worked correctly:

```{r}
head(total_edges)
```

## 3. Train a binary classifier to predict the links, i.e., to predict the class (positive/negative) using those heuristics. Use crossvalidation.

Our goal now is to train a model that can predict whether a link exists (`obs = 1`) or does not exist (`obs = 0`) between two nodes in the network. Thus, this a binary classification problem, where we try to classify edges as either "real" or "non-existent". For this we use a logistic regression model (`glm` with a binomial family) to learn the relationship between the link's existence (`obs`) and the heuristics we calculated (`sim_jacc`, `sim_aa`, `sim_pref`). Basically, the model learns which similarity metrics are the most predictive of real links.

First, we create train and test sets with a random sample split of 80% of the rows. We will use the training set to train the logistic regression model, and the testing set to predict new links. Lastly, we will evaluate the models performance with cross validation.

```{r train/test sets}
## Create train and test sets:
set.seed(123)  

# 80% sample
train_idx <- sample(1:nrow(total_edges), size = 0.8 * nrow(total_edges))

total_edges_train <- total_edges[train_idx, ]
total_edges_test <- total_edges[-train_idx, ]

# check check goose 
nrow(total_edges_train)  # Should be around 8000
nrow(total_edges_test)   # Should be around 2000
```

Followingly, we build the GLM logistic regression model with the heuristics as predictors. We use a `binomial(link = "logit")` family as we are doing a binary classification.

```{r GLM log regs model, results='asis'}
## Build logit model using binomial family from glm
glm_link <- glm(obs ~ sim_jacc + sim_aa + sim_pref,
                family = binomial(link = "logit"),
                data = total_edges_train)

summary(glm_link)

require(stargazer) # TODO: he likes to use stargazer and recommended it in class!
# stargazer(glm_link, type = "html", single.row = T, ...)

```

TODO: small interpretation?

Now we use `predict.glm()` to predict the probability for each link in the testing set with `type = "response"` to do the output on the probability scale (between 0 and 1).

```{r GLM prediction}
## Predict on the test set from the trained model
glm_prediction <- predict.glm(glm_link, newdata = total_edges_test, type = "response")
```

Finally we can evaluate the models accuracy using cross validation to see how well it performs with previously unseen data. The package `caret` makes it very easy to use confusion matrices. We initially set a threshold of 0.3 for classification of real or non-existent edge, which means that ... TODO. We are going to function this as we will be testing different thresholds below.

```{r evaluation function}
require(caret)

## Get confusion matrix based on prediction threshold

evaluation <- function(threshold, glm_prediction, total_edges_test) {
  
  # binary prediction based on threshold
  pred <- sign(glm_prediction > threshold)
  
  # confusion matrix based on threshold
  result <- confusionMatrix(factor(pred), factor(total_edges_test$obs), positive = "1")
  
  # cute lil message
  cat("Threshold Used:", threshold, "\n")
  
  return(result)
  
}

evaluation(0.3, glm_prediction, total_edges_test)
```

The confusion matrix shows us that the model does not predict a single non-existent link as actually non-existent (true negative = 0) as all predictions are actually classified as 1. Also, with an accuracy of 0.4895, our model is actually performing worse than random guessing, which is very lol.

All in all, our model predictions have a very serious issue.

We can try to solve this by choosing a different threshold, for example 0.5. (TODO: justification of why we do this)

```{r}
evaluation(0.5, glm_prediction, total_edges_test)
```

With this threshold, our model looks more normal. It has an accuracy of 63.45%, which is thankfully above random guessing. It does pretty good identifying actual non-existent links as non-existent links (specificity (recall for class 0) of 91.38%). It does however also predict a very high number of existent links as non-existent (negative predictive value of 59.20%). The worst metric is the sensitivity (recall for class 1 of 34.32%), where the model only captures around a third of the true links (it predicted of the true links 643 as negative and only 336 as positive).

All in all, our model right now is good at identifying the absence of links but struggles hevaily at predicting real links, showing a bias towards non-links (`obs`= 0)

Let's try changing our threshold again.

```{r}
evaluation(0.60, glm_prediction, total_edges_test)


# TODO: if we wanna go crazy we can do a ROC curve to get the best threshold. dunno dontcare you girls decide 

library(pROC)

# Generate ROC curve
roc_obj <- roc(total_edges_test$obs, glm_prediction)

# Plot the ROC curve
plot(roc_obj, main = "ROC Curve for Link Prediction")

# Find the optimal threshold (Youden's J Statistic)
opt_threshold <- coords(roc_obj, "best", ret = "threshold")
opt_threshold

evaluation(opt_threshold, glm_prediction, total_edges_test)
```

## 4. Evaluate the precision of the model. Which heuristic is the most important. Why do you think it is the most important?

Miau

## 5. Comment on potential ways to improve the link prediction

miaumiaumiau

# Conclusion
