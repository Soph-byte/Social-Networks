---
title: "third_assignment"
format: revealjs
editor: visual
---

# Introduction

In this project, we analyze a bipartite network that represents interactions between escorts and clients in Brazil. The data comes from an online platform where clients rated their experiences with escorts. In the network, one type of node stands for escorts, and the other for clients.

Each edge represents an encounter, with a rating attached to it: -1 (bad), 0 (neutral), or +1 (good). Some additional information like timestamps is also included in the dataset.

The aim of the project is to apply link prediction techniques to this network — in other words, to try and predict whether a connection between two nodes should exist or not, based on the structure of the rest of the network. To do this, we use similarity measures between nodes and train a binary classifier.

Since this is a bipartite network, meaning that edges only connect nodes of different types, this might influence how well certain similarity metrics perform.

Dataset source: <https://networks.skewed.de/net/escorts>

Reference: Rocha, L., Liljeros, F., & Holme, P. (2011). Simulated epidemics in an empirical spatiotemporal network of 50,185 sexual contacts. PLoS Computational Biology, 7(3). <https://doi.org/10.1371/journal.pcbi.1001109>

## Libraries

```{r}
library(readr)              # read csv 
library(igraph)             # create graph
library(dplyr)
library(ggplot2)
library(purrr)
```

## Network

We load the edges and nodes and create the graph from the data.

```{r loading network}
edges <- read_csv("network_escorts/edges.csv")
head(edges, 4)

nodes <- read_csv("network_escorts/nodes.csv")
head(nodes, 4)

# Buiding graph from data
g <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)
```

### Exercise 1

Task: Find the theoretical epidemic threshold βc for your network for the information to reach a significant number of nodes.

Since our network is bipartite (clients ↔ escorts), we cannot apply epidemic models directly. To simulate a realistic spreading scenario between individuals, we construct a unipartite projection of the bipartite graph, focusing on the client side. In this projection, two clients are connected if they have interacted with the same escort.

```{r}
colnames(nodes)
```

bipartite network, work with its bipartite projection instead of the raw bipartite network.

```{r}
V(g)$type <- V(g)$male == 0

is_bipartite(g)  
table(V(g)$type)
```

We then generate the bipartite projection and retain only the largest connected component of the client-client network for further analysis:

```{r}

proj <- bipartite_projection(g)
client_net <- proj$proj2  # proj2 = FALSE = clients


components <- components(client_net)
giant_client_net <- induced_subgraph(client_net, which(components$membership == which.max(components$csize)))

summary(giant_client_net)
plot(degree_distribution(giant_client_net), log = "xy", type = "l",
     main = "Degree distribution (clients projection)")
```

We compute β using the known approximation formula: $$ \beta_c = \mu \frac{\langle k \rangle}{\langle k^2 \rangle - \langle k \rangle}$$

μ=0.1 is the recovery rate ⟨k⟩ is the average degree ⟨k\^2⟩ is the average of the squared degrees

```{r}
mu <- 0.1
k <- degree(giant_client_net)          # our network
k_avg <- mean(k)
k_sq_avg <- mean(k^2)
beta_c <- mu * k_avg / (k_sq_avg - k_avg)
beta_c

```

This gives us the theoretical epidemic threshold that will guide our later simulations. beta_c= 0.0002004398

### Exercise 2

task: Assuming that randomly-selected 1% initial spreaders, simulate the SIR model below and above that threshold and plot the number of infected people as a function of β.

We define the function sim_sir() to simulate one realization of the spreading process. The function follows the standard SIR dynamics:

Nodes can be in one of three states: 0 = Susceptible, 1 = Infected, 2 = Recovered At each time step: Infected nodes recover with probability μ Susceptible neighbors of infected nodes get infected with probability β

```{r}
sim_sir <- function(g, beta, mu, seeds){
  state <- rep(0, vcount(g))  # 0 = S
  state[seeds] <- 1           # 1 = I
  t <- 0
  table <- data.frame(t=0, inf=seeds)
  while(sum(state == 1) > 0){
    t <- t + 1
    infected <- which(state == 1)
    state[infected] <- ifelse(runif(length(infected)) < mu, 2, 1)  # Recovery step
    infected <- which(state == 1)
    susceptible <- which(state == 0)
    contacts <- as.numeric(unlist(adjacent_vertices(g, infected)))
    contacts <- contacts[contacts %in% susceptible]
    new_infected <- contacts[runif(length(contacts)) < beta]
    if(length(new_infected) > 0){
      state[new_infected] <- 1
      table <- rbind(table, data.frame(t, inf=new_infected))
    }
  }
  table
}

```

We simulate the process for six different β values — ranging from below βc to far above — to compare how the infection curve evolves. Each simulation starts with 1% of randomly selected nodes as initial spreaders.

```{r}

generate_seeds <- function(g) {
  sample(1:vcount(g), vcount(g) * 0.01)
}

# different beta-values 
beta_vals <- c(0.0001, beta_c, 0.001, 0.005, 0.01, 0.05)

# simulation for each beat value
results <- map_dfr(beta_vals, function(beta){
  seeds <- generate_seeds(giant_client_net)
  sim <- sim_sir(giant_client_net, beta, mu, seeds)
  sim_summary <- sim %>% group_by(t) %>% summarize(ninf = n())
  sim_summary$beta <- beta
  sim_summary
})

```

visualization how the number of new infections per time step evolves for each β:

```{r}
ggplot(results, aes(x=t, y=ninf, color=factor(beta))) +
  geom_line() +
  labs(title = "SIR Simulation at Different Beta Values",
       x = "Time (t)", y = "Number of Infections",
       color = "Beta") +
  theme_minimal()

```

-   For $\beta < \beta_c$: The infection quickly dies out.
-   At $\beta \approx \beta_c$: A limited spread may occur.
-   For $\beta \gg \beta_c$: A large portion of the network becomes infected rapidly, with an early peak and a fast decline as nodes recover.

The simulation confirms the theoretical threshold $\beta_c$ and shows how the dynamics of spreading change as a function of $\beta$.

### Exercise 3

Task: Choose a well-above above . Using centrality , communities or any other β βc suitable metric, find a better set of 1% of seeds in the network so we get more infected people than the random case. Measure the difference of your choice with the random case as: a)The difference in the total number of infected people:

```{r}
 # Parameters
mu <- 0.1
beta_high <- 0.01  # well above beta_c

# 1% random seeds
random_seeds <- sample(1:vcount(giant_client_net), vcount(giant_client_net) * 0.01)

# Top 1% by closeness centrality
centrality <- closeness(giant_client_net)
centrality_df <- data.frame(node = 1:vcount(giant_client_net), score = centrality)
top_seeds <- centrality_df %>%
  arrange(desc(score)) %>%
  slice_head(n = round(vcount(giant_client_net) * 0.01)) %>%
  pull(node)

# Run SIR simulations
sim_random <- sim_sir(giant_client_net, beta_high, mu, random_seeds)
sim_targeted <- sim_sir(giant_client_net, beta_high, mu, top_seeds)

# a) Total number of infected nodes
total_random <- sim_random %>% distinct(inf) %>% nrow()
total_targeted <- sim_targeted %>% distinct(inf) %>% nrow()
diff_total <- total_targeted - total_random

cat("Random total infected:", total_random, "\n")
cat("Targeted total infected:", total_targeted, "\n")
cat("Difference:", diff_total, "more infected with centrality-based seeding\n")


```

b)The difference in the time of the peak of infection (when most infections happen):

```{r}
# Run again and group by time
sim_random <- sim_sir(giant_client_net, beta_high, mu, random_seeds) %>%
  group_by(t) %>% summarize(ninf = n())

sim_targeted <- sim_sir(giant_client_net, beta_high, mu, top_seeds) %>%
  group_by(t) %>% summarize(ninf = n())

# Time of peak (max infections)
peak_random <- sim_random$t[which.max(sim_random$ninf)]
peak_targeted <- sim_targeted$t[which.max(sim_targeted$ninf)]
diff_peak <- peak_random - peak_targeted

cat("Peak (random):   t =", peak_random, "\n")
cat("Peak (targeted): t =", peak_targeted, "\n")
cat("Difference in peak time:", diff_peak, "time steps earlier with targeted seeding\n")


```

### Exercise 4

Using the same , design a “quarantine strategy”: at time step or , β t = 3 4 quarantine 20 % of the susceptible population. You can model quarantine by 8 temporally removing these nodes. Release the quarantined nodes time steps later, making them susceptible again. Measure the difference with respect to no quarantine.

```{r}
sim_sir_quarantine <- function(g, beta, mu, seeds, quarantine_t = 4, quarantine_frac = 0.2, quarantine_duration = 8){
  state <- rep(0, vcount(g))  # 0 = S, 1 = I, 2 = R, 3 = Quarantined (temporary)
  state[seeds] <- 1
  t <- 0
  table <- data.frame(t=0, inf=seeds)
  
  quarantine_nodes <- c()
  quarantine_release_t <- NA
  
  while(sum(state == 1) > 0 || any(state == 3)){
    t <- t + 1

    # Recover infected
    infected <- which(state == 1)
    state[infected] <- ifelse(runif(length(infected)) < mu, 2, 1)
    
    # Quarantine: at time t, choose 20% of susceptible
    if (t == quarantine_t) {
      susceptible <- which(state == 0)
      quarantine_nodes <- sample(susceptible, round(length(susceptible) * quarantine_frac))
      state[quarantine_nodes] <- 3  # Mark as quarantined
      quarantine_release_t <- t + quarantine_duration
    }

    # Release quarantine
    if (!is.na(quarantine_release_t) && t == quarantine_release_t) {
      state[quarantine_nodes] <- 0  # Back to susceptible
    }

    # Infection
    infected <- which(state == 1)
    susceptible <- which(state == 0)
    contacts <- as.numeric(unlist(adjacent_vertices(g, infected)))
    contacts <- contacts[contacts %in% susceptible]
    new_infected <- contacts[runif(length(contacts)) < beta]
    if (length(new_infected) > 0){
      state[new_infected] <- 1
      table <- rbind(table, data.frame(t = t, inf = new_infected))
    }
  }
  table
}

```

```{r}
# Reuse seeds
set.seed(123)
seeds <- sample(1:vcount(giant_client_net), vcount(giant_client_net) * 0.01)

# Run without quarantine
no_q <- sim_sir(giant_client_net, beta_high, mu, seeds) %>%
  group_by(t) %>% summarize(ninf = n()) %>%
  mutate(strategy = "No Quarantine")

# Run with quarantine at t = 4, for 8 steps, 20% of susceptibles
q <- sim_sir_quarantine(giant_client_net, beta_high, mu, seeds,
                        quarantine_t = 4, quarantine_frac = 0.2, quarantine_duration = 8) %>%
  group_by(t) %>% summarize(ninf = n()) %>%
  mutate(strategy = "Quarantine")

# Combine
quarantine_results <- bind_rows(no_q, q)

```

```{r}
ggplot(quarantine_results, aes(x = t, y = ninf, color = strategy)) +
  geom_line() +
  labs(title = "Effect of Quarantine Strategy on Infection Spread",
       x = "Time (t)", y = "Number of Infections",
       color = "Strategy") +
  theme_minimal()

```

### Exercise 5

Suppose now that you can convince 5% of people in the network not to spread that information at all. - Choose those 5% randomly in the network. Simulate the SIR model above βc using 1% of the remaining nodes as seeds. Choose those seeds randomly. - Choose those 5% according to their centrality. Simulate the SIR model using 1% of the remaining nodes as seeds. Choose those seeds above βc randomly. 6. 7. - Measure the difference between both cases as you did in part c).

```{r}
mu <- 0.1
beta_block <- 0.01  # well above beta_c
n_total <- vcount(giant_client_net)
n_block <- round(n_total * 0.05)
n_seeds <- round((n_total - n_block) * 0.01)

```

```{r}
# Random block
set.seed(123)
blocked_random <- sample(1:n_total, n_block)
g_rand_blocked <- delete_vertices(giant_client_net, blocked_random)

# Random seeds from remaining nodes
seeds_rand <- sample(1:vcount(g_rand_blocked), n_seeds)

# SIR simulation
sim_rand_blocked <- sim_sir(g_rand_blocked, beta_block, mu, seeds_rand)

```

```{r}
# Closeness centrality
centrality <- closeness(giant_client_net)
top_blocked <- order(centrality, decreasing = TRUE)[1:n_block]
g_central_blocked <- delete_vertices(giant_client_net, top_blocked)

# Seeds from remaining nodes
seeds_central <- sample(1:vcount(g_central_blocked), n_seeds)

# SIR simulation
sim_central_blocked <- sim_sir(g_central_blocked, beta_block, mu, seeds_central)

```

```{r}
total_inf_rand <- sim_rand_blocked %>% distinct(inf) %>% nrow()
total_inf_central <- sim_central_blocked %>% distinct(inf) %>% nrow()
diff_total_inf <- total_inf_rand - total_inf_central

cat("Random block infected:", total_inf_rand, "\n")
cat("Centrality block infected:", total_inf_central, "\n")
cat("Difference:", diff_total_inf, "fewer infections with centrality-based blocking\n")

```

```{r}
peak_rand <- sim_rand_blocked %>% group_by(t) %>% summarize(ninf = n()) %>%
  filter(ninf == max(ninf)) %>% pull(t)

peak_central <- sim_central_blocked %>% group_by(t) %>% summarize(ninf = n()) %>%
  filter(ninf == max(ninf)) %>% pull(t)

diff_peak_time <- peak_rand - peak_central

cat("Peak (random):", peak_rand, "\n")
cat("Peak (centrality):", peak_central, "\n")
cat("Difference in peak time:", diff_peak_time, "steps\n")

```

### Exercise 6

Comment on the relationship between the findings in part c) and d) using the same type of centrality for the 1% in part c) and 5% in part d)

In both experiments (part c and part d), we used **closeness centrality** to identify highly influential nodes in the network. Interestingly, the same centrality measure enabled two opposite strategies:

-   In part **c**, selecting the top 1% central nodes as **initial spreaders** significantly **accelerated and amplified** the spread. These nodes, due to their central position, could reach other parts of the network quickly and efficiently.

-   In part **d**, removing the top 5% central nodes **weakened** the network's connectivity. As a result, the spread was **slower and less extensive**, confirming the importance of these nodes in information diffusion.

This contrast highlights how centrality can be used both to **maximize** and to **mitigate** the spread — depending on whether the goal is **promotion** or **prevention** of information flow.

Exercise 7

With the results of part b) train a model that predicts that time to infection of a node using their degree, centrality, betweeness, page rank and any other predictors you see fit. Use that model to select the seed nodes as those with the smallest time to infection in part c). Repeat d).

```{r}
# One full SIR simulation
set.seed(123)
seeds <- sample(1:vcount(giant_client_net), vcount(giant_client_net) * 0.01)
sim_data <- sim_sir(giant_client_net, beta = 0.01, mu = 0.1, seeds)

# Features
df <- data.frame(
  node = 1:vcount(giant_client_net),
  degree = degree(giant_client_net),
  closeness = closeness(giant_client_net),
  betweenness = betweenness(giant_client_net),
  pagerank = page_rank(giant_client_net)$vector
)

# Merge with infection time
df <- df %>%
  left_join(sim_data, by = c("node" = "inf")) %>%
  mutate(t = ifelse(is.na(t), max(sim_data$t) + 1, t))  # assign max+1 to uninfected


```

```{r}
model <- lm(t ~ degree + closeness + betweenness + pagerank, data = df)
summary(model)

```

```{r}
df$predicted_t <- predict(model, df)
predicted_seeds <- df %>%
  arrange(predicted_t) %>%
  slice_head(n = round(vcount(giant_client_net) * 0.01)) %>%
  pull(node)

```

```{r}
sim_predicted <- sim_sir(giant_client_net, beta = 0.01, mu = 0.1, predicted_seeds)

```

```{r}
sim_rand <- sim_sir(giant_client_net, beta = 0.01, mu = 0.1,
                    sample(1:vcount(giant_client_net), vcount(giant_client_net) * 0.01))

sim_central <- sim_sir(giant_client_net, beta = 0.01, mu = 0.1, top_seeds)

```

```{r}
prepare_plot <- function(sim, label) {
  sim %>% group_by(t) %>% summarize(ninf = n()) %>% mutate(type = label)
}

df_all <- bind_rows(
  prepare_plot(sim_rand, "Random"),
  prepare_plot(sim_central, "Centrality"),
  prepare_plot(sim_predicted, "Predicted")
)

ggplot(df_all, aes(x = t, y = ninf, color = type)) +
  geom_line() +
  labs(title = "SIR Spread by Seed Strategy",
       x = "Time", y = "New Infections", color = "Seeding Strategy") +
  theme_minimal()

```

We trained a linear model to predict the infection time of each node using features such as: - Degree - Closeness - Betweenness - PageRank

We then selected the 1% of nodes with the **lowest predicted infection time** as seeds and compared the SIR spread to two baseline strategies: - **Randomly selected seeds** - **Seeds based on closeness centrality**

**Results:** - The predicted seeding strategy produced a spread curve **very similar** to the centrality-based one. - Both strategies clearly outperformed the random baseline in terms of **early infection** and **infection peak size**.

This suggests that **machine learning models trained on network features can approximate good seed selection** — even without manually choosing a specific centrality measure.

The model explained approximately **30% of the variation** in infection timing (( R\^2 = 0.30 )), which is a solid result given the stochastic nature of the SIR process.

Among the predictors, **closeness centrality** and **PageRank** had the strongest (negative) effect on infection time. This confirms that nodes that are **well-connected and centrally located** in the network tend to get infected earlier.

These findings validate the idea that **network centrality can be used to guide both spreading and suppression strategies**, and show that machine learning can help identify influential nodes automatically.
